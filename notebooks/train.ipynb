{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWUC2t3_nYvf"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "!gdown 10G5-q3Eq52tYEncTF54douBsjCTUVoXb\n",
        "!unrar x datasets.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwaxFpNM2VuK",
        "outputId": "6026e151-c2e5-48e2-977d-b30a2d643fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting av\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-10.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install av\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT0KJrZWGvqB"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import av\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import TimesformerConfig, TimesformerForVideoClassification, AutoImageProcessor\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efGN7J5-OXMA"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RObvYeSTGw6E",
        "outputId": "0a08840e-4b57-45fd-8034-1956d701b4f4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-84668652-0de1-49eb-9d67-147ee36834a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>youtube_id</th>\n",
              "      <th>time_start</th>\n",
              "      <th>time_end</th>\n",
              "      <th>split</th>\n",
              "      <th>path</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>belly dancing</td>\n",
              "      <td>-2JgjPsy4Eo</td>\n",
              "      <td>77</td>\n",
              "      <td>87</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/Anna Rubtsova - Zay El Asal ISA...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>salsa dancing</td>\n",
              "      <td>-3FihEVl-R8</td>\n",
              "      <td>26</td>\n",
              "      <td>36</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/Salsa dancing Hassan and Kim.3gpp</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>country line dancing</td>\n",
              "      <td>-4HzfnOtVeI</td>\n",
              "      <td>150</td>\n",
              "      <td>160</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/run rabbit run Line Dance par J...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tap dancing</td>\n",
              "      <td>-4r6VLqGeK4</td>\n",
              "      <td>106</td>\n",
              "      <td>116</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/My Mad Tap Dancing Skillz c.3gpp</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mosh pit dancing</td>\n",
              "      <td>-9N39otwJl8</td>\n",
              "      <td>22</td>\n",
              "      <td>32</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/mosh pitts.3gpp</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2185</th>\n",
              "      <td>dancing ballet</td>\n",
              "      <td>zqsDdWv33Ho</td>\n",
              "      <td>102</td>\n",
              "      <td>112</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/ballet CascaNueces Juguetes.3gpp</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2186</th>\n",
              "      <td>mosh pit dancing</td>\n",
              "      <td>zsY9bKeIW9o</td>\n",
              "      <td>48</td>\n",
              "      <td>58</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/Pierce The Veil Mosh Pit.3gpp</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2187</th>\n",
              "      <td>belly dancing</td>\n",
              "      <td>zvni26d4ZoI</td>\n",
              "      <td>102</td>\n",
              "      <td>112</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/Asian girl belly dancing in a l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2188</th>\n",
              "      <td>mosh pit dancing</td>\n",
              "      <td>zxhj8Mg2oNE</td>\n",
              "      <td>13</td>\n",
              "      <td>23</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/Foo Fighters Live at Wembley Mo...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189</th>\n",
              "      <td>tango dancing</td>\n",
              "      <td>zzOkaJlTbKU</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>train</td>\n",
              "      <td>datasets/train/Finał kategoria pow15 B Tańce s...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2190 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84668652-0de1-49eb-9d67-147ee36834a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-84668652-0de1-49eb-9d67-147ee36834a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-84668652-0de1-49eb-9d67-147ee36834a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     label   youtube_id  time_start  time_end  split  \\\n",
              "0            belly dancing  -2JgjPsy4Eo          77        87  train   \n",
              "1            salsa dancing  -3FihEVl-R8          26        36  train   \n",
              "2     country line dancing  -4HzfnOtVeI         150       160  train   \n",
              "3              tap dancing  -4r6VLqGeK4         106       116  train   \n",
              "4         mosh pit dancing  -9N39otwJl8          22        32  train   \n",
              "...                    ...          ...         ...       ...    ...   \n",
              "2185        dancing ballet  zqsDdWv33Ho         102       112  train   \n",
              "2186      mosh pit dancing  zsY9bKeIW9o          48        58  train   \n",
              "2187         belly dancing  zvni26d4ZoI         102       112  train   \n",
              "2188      mosh pit dancing  zxhj8Mg2oNE          13        23  train   \n",
              "2189         tango dancing  zzOkaJlTbKU           2        12  train   \n",
              "\n",
              "                                                   path  target  \n",
              "0     datasets/train/Anna Rubtsova - Zay El Asal ISA...       0  \n",
              "1      datasets/train/Salsa dancing Hassan and Kim.3gpp      10  \n",
              "2     datasets/train/run rabbit run Line Dance par J...       2  \n",
              "3       datasets/train/My Mad Tap Dancing Skillz c.3gpp      14  \n",
              "4                        datasets/train/mosh pitts.3gpp       8  \n",
              "...                                                 ...     ...  \n",
              "2185    datasets/train/ballet CascaNueces Juguetes.3gpp       3  \n",
              "2186       datasets/train/Pierce The Veil Mosh Pit.3gpp       8  \n",
              "2187  datasets/train/Asian girl belly dancing in a l...       0  \n",
              "2188  datasets/train/Foo Fighters Live at Wembley Mo...       8  \n",
              "2189  datasets/train/Finał kategoria pow15 B Tańce s...      13  \n",
              "\n",
              "[2190 rows x 7 columns]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import glob\n",
        "valid_paths = glob.glob(\"datasets/train/*\")\n",
        "\n",
        "data = pd.read_csv('datasets/kinetics700/train_dance.csv')\n",
        "data = data.dropna(subset=['path'])\n",
        "data['path'] = [i.replace('mp4', '3gpp') for i in data['path']]\n",
        "data['path'] = [i.replace('\\\\', r'/') for i in data['path']]\n",
        "data = data[[i in valid_paths for i in data.path]].copy()\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "data['target'] = le.fit_transform(data.label)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "9XreSfXdu3gK"
      },
      "outputs": [],
      "source": [
        "def read_video_opencv(capture, indices):\n",
        "\n",
        "    frames = []\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    idx = 0\n",
        "    while True:\n",
        "        ret, frame = capture.read()\n",
        "        if ret:\n",
        "            if idx > end_index:\n",
        "                capture.release()\n",
        "                break\n",
        "            if idx >= start_index and idx in indices:\n",
        "                frames.append(frame[:, :, ::-1])\n",
        "            idx += 1\n",
        "    capture.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = converted_len\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "aXkd8BjaOckv"
      },
      "outputs": [],
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.paths = data['path']\n",
        "        self.targets = data['target']\n",
        "        self.time_start = data['time_start']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.paths.iloc[idx]\n",
        "        target = self.targets.iloc[idx]\n",
        "\n",
        "        container = av.open(file_path)\n",
        "        cap = cv2.VideoCapture(file_path)\n",
        "\n",
        "        start_idx = self.time_start.iloc[idx] * cap.get(cv2.CAP_PROP_FPS)\n",
        "        if start_idx < cap.get(cv2.CAP_PROP_FPS):  # Некорректная разметка на некоторых примерах\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, start_idx)\n",
        "\n",
        "        try:\n",
        "            indices = sample_frame_indices(clip_len=8, frame_sample_rate=5, seg_len=cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        except Exception:\n",
        "            indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if indices.shape[0] < 8:\n",
        "            indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        video = read_video_opencv(cap, indices)\n",
        "        while video.shape[0] < 8:\n",
        "            video = np.vstack([video, video[-1:]])\n",
        "\n",
        "        video = processor(list(video), return_tensors=\"pt\")\n",
        "        return video, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "UUQwa3a2d8ai"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = train_test_split(data, test_size=0.3, shuffle=True, stratify=data['target'])\n",
        "train_dataset = VideoDataset(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk4HRIBzwbeC",
        "outputId": "28fe3c6a-4c40-4afc-bead-df12156ead00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4 #8\n",
        "processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.3, shuffle=True, stratify=data['target'])\n",
        "\n",
        "train_dataset = VideoDataset(train_data)\n",
        "val_dataset = VideoDataset(val_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF-yyFj6D6iZ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9PLDln30HUzq",
        "outputId": "2a7ad307-ae02-4048-9f31-4ee258c9329c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ7ZILYdD4kY",
        "outputId": "56127522-b14a-4a06-c26d-26312469cff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TimesformerForVideoClassification(\n",
              "  (timesformer): TimesformerModel(\n",
              "    (embeddings): TimesformerEmbeddings(\n",
              "      (patch_embeddings): TimesformerPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (time_drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): TimesformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x TimesformerLayer(\n",
              "          (drop_path): Identity()\n",
              "          (attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): TimesformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): TimesformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# configuration = TimesformerConfig()\n",
        "# model = TimesformerForVideoClassification(configuration)\n",
        "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "model.classifier = nn.Linear(in_features=768, out_features=data.label.nunique(), bias=True)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "kZFWQqcdEVmb"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "_21jcWyGzMRI"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "uNUwt6N3EEE1"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    for i, data in tqdm(enumerate(train_loader)):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs['pixel_values'].to(device)\n",
        "        inputs = torch.squeeze(inputs, 1)\n",
        "\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predicted_label = logits.softmax(dim=1)\n",
        "\n",
        "        loss = loss_fn(logits, labels) # predicted_label\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    last_loss = running_loss / len(train_dataset)\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7SBo2UrED8v",
        "outputId": "714499a0-e3ac-47a0-88ec-389ef6ad81e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:36,  2.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.7014194256918771 valid 0.610669777393341\n",
            "F1: 0.23384612381537984\n",
            "EPOCH 2:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:35,  2.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.3649964950765882 valid 0.5969261852900187\n",
            "F1: 0.26460454835841835\n",
            "EPOCH 3:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:36,  2.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.1284137983407293 valid 0.6627759162584941\n",
            "F1: 0.21414853768040917\n",
            "EPOCH 4:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:35,  2.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.06576877956145576 valid 0.7047584621111552\n",
            "F1: 0.25903934842317383\n",
            "EPOCH 5:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:39,  2.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.0298586087860167 valid 0.7592365447680155\n",
            "F1: 0.22805174588485125\n",
            "EPOCH 6:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:34,  2.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.01681288529586579 valid 0.670877077182134\n",
            "F1: 0.28881621923727185\n",
            "EPOCH 7:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:33,  2.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.015929679339086372 valid 0.7130652360121409\n",
            "F1: 0.3367013963985293\n",
            "EPOCH 8:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:34,  2.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.015703272059160684 valid 0.7141064500808716\n",
            "F1: 0.24009956077765343\n",
            "EPOCH 9:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:33,  2.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.01253942341277642 valid 0.7584423327445984\n",
            "F1: 0.2489476949240995\n",
            "EPOCH 10:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "88it [03:33,  2.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 0.003513576736108267 valid 0.7884823719660441\n",
            "F1: 0.2566415350686373\n"
          ]
        }
      ],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "best_vloss = 1000000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    model.train()\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    val_targets = []\n",
        "    val_preds = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "          inputs, labels = data\n",
        "          inputs = inputs['pixel_values'].to(device)\n",
        "          inputs = torch.squeeze(inputs, 1)\n",
        "\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          logits = outputs.logits\n",
        "          predicted_label = logits.softmax(dim=1)\n",
        "          vloss = loss_fn(logits, labels)\n",
        "          running_vloss += vloss.item()\n",
        "          val_targets.extend(labels.cpu().numpy())\n",
        "          val_preds.extend(predicted_label.argmax(axis=1).cpu().numpy())\n",
        "\n",
        "    avg_vloss = running_vloss / len(val_dataset)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "    print('F1:', f1_score(val_targets, val_preds, average='macro'))\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
